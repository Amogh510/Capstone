## Complete Test Generation Pipeline â€” Stages 1, 2A & 3

Implements the complete UI Test Case Generation Pipeline with **Multi-Agent System**:
- **Stage 1A**: KG Entity Extraction (Routes + Components) via semantic similarity
- **Stage 1B**: LLM-based Structured Scenario Interpretation
- **Stage 2A**: Subgraph Retrieval (Context Extraction from KG)
  - **NEW**: Multi-Agent Intelligent Retrieval with iterative expansion
- **Stage 3**: E2E Test Case Generation (Playwright/Cypress/Selenium)
  - **NEW**: Multi-Agent Generation with Planner, Architect, Writer, Reviewer, and Refinement

---

## ðŸš€ NEW: Multi-Agent Architecture

The enhanced system features a sophisticated **multi-agent workflow** that dramatically improves test quality through intelligent collaboration and iterative refinement.

### Key Features

- **ðŸ¤– Multi-Agent KG Retrieval**: Intelligent multi-hop graph traversal with priority-guided expansion
- **ðŸ“‹ Test Planning Agent**: Creates comprehensive test strategies and coverage goals
- **ðŸ—ï¸ Test Architect Agent**: Designs test structure, fixtures, and assertion strategies
- **âœï¸ Test Writer Agent**: Generates high-quality test code/specs
- **ðŸ” Reviewer Agent**: Evaluates quality and provides detailed feedback
- **ðŸ”„ Iterative Refinement**: Tests improved through multiple iterations until quality threshold met

**See [MULTI_AGENT_ARCHITECTURE.md](./MULTI_AGENT_ARCHITECTURE.md) for complete documentation.**

### Quick Start - Multi-Agent Pipeline

```bash
# Basic usage
python multi_agent_pipeline.py "User logs in and views dashboard"

# High-quality production tests
python multi_agent_pipeline.py "User registers and completes profile" \
  --retrieval-iterations 4 \
  --refinement-iterations 3 \
  --quality-threshold 0.85 \
  --framework playwright \
  --format both

# See examples
python example_multi_agent.py --example all
```

---

## Original Pipeline (Still Available)

### Features

#### Stage 1A: Multi-Stage Retrieval
- **Global query search**: Embeds entire scenario for holistic understanding
- **Per-step search**: Individual embeddings for each step with position-aware weighting
- **Keyword boosting**: Additional scoring for exact route matches
- Neo4j vector index search (preferred if indexes exist)
- Local cosine-similarity fallback with cached embeddings
- LangChain embeddings (HuggingFace) by default

#### Stage 1B: LLM Structured Interpretation
- Uses retrieved KG entities to produce structured JSON output
- Supports any LiteLLM-compatible model (GPT-4, Claude, Groq, etc.)
- Validates scenario type, required routes/components, auth requirements
- Determines priority based on workflow complexity

#### Stage 2A: Subgraph Retrieval (Smart Filtering)
- **ðŸŽ¯ Smart Filtering**: Removes noise while keeping test-relevant context
  - **Minimal mode** (default): 82 nodes, 94 edges (78% reduction)
  - Focuses on: Components, Routes, States, EventHandlers, Hooks, Props
  - Excludes: Styling nodes (TailwindUtility, InlineStyle), deep JSX trees
- **ðŸ“Š Three filtering levels**:
  - **Minimal**: Test-relevant entities only (recommended for test generation)
  - **Smart Filtered**: Balanced view with aggregated styling
  - **Full**: Complete graph with all details
- **ðŸŽ¨ Styling aggregation**: Summarizes Tailwind classes instead of 1300+ individual nodes
- **ðŸŒ³ JSX depth limiting**: Prevents overwhelming DOM tree details
- **Configurable depth traversal**: Control how deep to explore relationships (default: depth=2)
- **Efficient graph queries**: Optimized Cypher queries for fast subgraph extraction
- **Summary statistics**: Node/edge counts, type breakdowns, and styling summaries

#### Stage 3: E2E Test Case Generation
- **ðŸ§ª Multi-Framework Support**: Generate tests for Playwright, Cypress, or Selenium
- **ðŸ¤– LLM-Powered**: Uses advanced language models (GPT-4, Claude, Llama, etc.) via LiteLLM
- **ðŸŽ¯ Context-Aware**: Leverages KG subgraph to generate accurate selectors and flows
- **ðŸ“ Production-Ready**: Generates complete, executable tests with proper structure
- **âœ… Comprehensive Assertions**: Includes navigation checks, state verification, and UI validation
- **ðŸ” Auth Support**: Automatically includes login flows when required
- **ðŸ“š Best Practices**: Follows framework-specific patterns and conventions
- **ðŸ’¡ Smart Selectors**: Prefers data-testid, role-based, and semantic selectors
- **âš¡ Robust Waiting**: Includes proper waits and error handling

### Environment

#### Required
- `NEO4J_URI` (default `bolt://localhost:7687`)
- `NEO4J_USER` (default `neo4j`)
- `NEO4J_PASS` or `NEO4J_PASSWORD` (default `admin123`)

#### Stage 1A Configuration
- `STAGE1_EMBEDDING_MODEL` (default `sentence-transformers/all-MiniLM-L6-v2`)
- `STAGE1_COMPONENT_INDEX` (default `component_embeddings`)
- `STAGE1_ROUTE_INDEX` (default `route_embeddings`)
- `STAGE1_TOPK_COMPONENTS` (default `5`)
- `STAGE1_TOPK_ROUTES` (default `5`)
- `STAGE1_CACHE_DIR` (default `.cache/stage1`)

#### Stage 1B Configuration (LLM)
- `LITELLM_MODEL` (e.g., `groq/llama-3.3-70b-versatile`, `gpt-4o-mini`)
- `GROQ_API_KEY` (if using Groq models)
- `OPENAI_API_KEY` (if using OpenAI models)
- `STAGE1_ENABLE_STAGE1B` (default `true`)
- `STAGE1_LLM_TEMPERATURE` (default `0.0`)
- `STAGE1_LLM_MAX_TOKENS` (default `500`)

#### Stage 2A Configuration (Subgraph Retrieval)
- `STAGE2_MAX_DEPTH` (default `2`)
- `STAGE2_INCLUDE_FILE_CONTEXT` (default `true`)

#### Stage 3 Configuration (Test Generation)
- `STAGE3_TEST_FRAMEWORK` (default `playwright`) - Options: `playwright`, `cypress`, `selenium`
- `STAGE3_LLM_MODEL` (e.g., `groq/llama-3.3-70b-versatile`, `gpt-4o-mini`)
- `STAGE3_TEMPERATURE` (default `0.1`) - LLM temperature for test generation
- `STAGE3_MAX_TOKENS` (default `2000`) - Max tokens for generated test
- `STAGE3_INCLUDE_COMMENTS` (default `true`) - Include inline comments
- `STAGE3_INCLUDE_ASSERTIONS` (default `true`) - Include assertions in tests

### Install (recommended: reuse retrieval-service venv or create a new one)
```bash
cd /Users/aneesh/Capstone/stage1
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Usage

---

## Multi-Agent Pipeline (Recommended)

### Command Line

```bash
# Basic multi-agent pipeline
python multi_agent_pipeline.py "Your scenario here"

# With custom settings
python multi_agent_pipeline.py "Login â†’ Dashboard â†’ Create Todo" \
  --retrieval-iterations 3 \
  --retrieval-strategy priority_guided \
  --framework playwright \
  --format both \
  --refinement-iterations 3 \
  --quality-threshold 0.8 \
  --output-dir ./test_outputs

# Different frameworks and formats
python multi_agent_pipeline.py "User workflow" --framework cypress --format spec
python multi_agent_pipeline.py "Admin flow" --framework selenium --format code
```

### Python API

```python
from stage1.multi_agent_pipeline import run_multi_agent_pipeline

result = run_multi_agent_pipeline(
    scenario="User logs in â†’ Views dashboard â†’ Creates todo",
    retrieval_iterations=3,
    retrieval_strategy="priority_guided",
    test_framework="playwright",
    output_format="both",
    refinement_iterations=3,
    quality_threshold=0.8
)

# Access outputs
test_plan = result['stage3']['test_plan']
test_code = result['stage3']['test_code']
test_specs = result['stage3']['test_specs']
final_quality = result['stage3']['metadata']['final_quality_score']
reviews = result['stage3']['reviews']
```

### Examples

```bash
# Run basic example
python example_multi_agent.py --example basic

# Run Cypress specs example
python example_multi_agent.py --example cypress

# Run Playwright code example
python example_multi_agent.py --example playwright

# Compare old vs new
python example_multi_agent.py --example comparison

# Run all examples
python example_multi_agent.py --example all
```

---

## Original Pipeline (Single-Pass)

### Stage 1 Only (CLI)
```bash
python service.py "Login â†’ Dashboard â†’ Todos CRUD" --topk-components 8 --topk-routes 8
```

#### Stage 2A Only (CLI)
```bash
# From Stage 1 output
python subgraph_retrieval.py --input-json stage1_output.json --depth 2

# Or directly with routes/components
python subgraph_retrieval.py --routes /dashboard /login --components Dashboard Login --depth 2
```

#### Complete Pipeline (Stage 1 + Stage 2A)
```bash
# Using the example script
python example_stage1_2a.py "Register â†’ Dashboard â†’ Analytics"
```

#### Stage 3 Only (Test Generation)
```bash
# From pipeline output (Stage 1 + 2A)
python test_generator.py --input pipeline_output.json --output tests/scenario.spec.ts

# With custom framework
python test_generator.py --input pipeline_output.json --framework cypress --output tests/scenario.cy.js
```

#### Complete End-to-End Pipeline (Stage 1 + 2A + 3)
```bash
# Basic usage - generates Playwright test
python example_end_to_end.py "Login â†’ Dashboard â†’ Create Todo"

# With Cypress framework
python example_end_to_end.py "Register â†’ Dashboard â†’ Analytics" --framework cypress

# Save to specific file
python example_end_to_end.py "User workflow" --output ./tests/user-workflow.spec.ts

# Custom depth and framework
python example_end_to_end.py "Complex flow" --depth 3 --framework playwright --output ./tests/complex.spec.ts
```

#### Python API

**Stages 1 + 2A:**
```python
from stage1.service import Stage1Service
from stage1.subgraph_retrieval import retrieve_subgraph

# Stage 1: Entity extraction and interpretation
svc = Stage1Service()
stage1_result = svc.run_stage1("Login â†’ Dashboard â†’ Todos CRUD")

# Stage 2A: Subgraph retrieval
subgraph = retrieve_subgraph(stage1_result['structured'], depth=2)

print(f"Retrieved {subgraph['summary']['node_count']} nodes")
print(f"Retrieved {subgraph['summary']['edge_count']} edges")
```

**Complete Pipeline (Stages 1 + 2A + 3):**
```python
from stage1.service import Stage1Service
from stage1.subgraph_retrieval import retrieve_subgraph, Stage2Config
from stage1.test_generator import generate_test, Stage3Config, TestFramework

# Stage 1: Scenario interpretation
svc = Stage1Service()
stage1_result = svc.run_stage1("Login â†’ Dashboard â†’ Create Todo")

# Stage 2A: Subgraph retrieval with minimal filtering
config2 = Stage2Config()
config2.include_only_node_types = ["Component", "Route", "State", "EventHandler", "Hook", "Prop"]
subgraph = retrieve_subgraph(stage1_result['structured'], depth=2, config=config2)

# Stage 3: Test generation
config3 = Stage3Config()
config3.test_framework = TestFramework.PLAYWRIGHT
test_result = generate_test(stage1_result['structured'], subgraph, config=config3)

# Save generated test
with open("tests/todo-workflow.spec.ts", "w") as f:
    f.write(test_result["test_code"])

print(f"âœ… Generated {test_result['framework']} test: {test_result['test_name']}")
print(f"   Assertions: {test_result['metadata']['assertions_count']}")
print(f"   Steps: {test_result['metadata']['steps_count']}")
```

---

## Configuration

### Multi-Agent System Configuration

#### Retrieval Configuration

```bash
export MULTI_AGENT_MAX_ITERATIONS=3              # Retrieval iterations (1-5)
export MULTI_AGENT_EXPANSION_DEPTH=2             # Hop depth per iteration
export MULTI_AGENT_STRATEGY=priority_guided      # or breadth_first
export MULTI_AGENT_MIN_RELEVANCE=0.3             # Filter threshold
export MULTI_AGENT_LLM_MODEL=groq/llama-3.3-70b-versatile
```

#### Test Generation Configuration

```bash
export MULTI_AGENT_TEST_LLM=groq/llama-3.3-70b-versatile
export TEST_FRAMEWORK=playwright                  # or cypress, selenium
export TEST_OUTPUT_FORMAT=both                    # code, spec, or both
export MAX_REFINEMENT_ITERATIONS=3                # Quality iterations
export QUALITY_THRESHOLD=0.8                      # Stop when reached
export NUM_TEST_CASES=5                           # Target test cases
```

See [MULTI_AGENT_ARCHITECTURE.md](./MULTI_AGENT_ARCHITECTURE.md) for detailed configuration guide.

---

## Architecture Comparison

| Feature | Old Pipeline | Multi-Agent Pipeline |
|---------|-------------|---------------------|
| KG Retrieval | Single-pass, fixed depth | Multi-hop, intelligent expansion |
| Node Selection | All nodes within depth | Priority-guided, filtered |
| Test Planning | None | Dedicated Planner Agent |
| Test Architecture | None | Dedicated Architect Agent |
| Generation | Single LLM call | Iterative with refinement |
| Quality Assessment | None | Reviewer Agent with scoring |
| Iterations | 1 | 3+ (configurable) |
| Avg Quality Score | 0.60-0.70 | 0.80-0.95 |
| Test Coverage | ~60% | ~90-95% |
| Time | ~7s | ~25-30s |
| Cost | ~$0.002 | ~$0.005-$0.01 |

**Recommendation**: Use multi-agent pipeline for production tests; use original pipeline for rapid prototyping.

---

## Files

### Core Pipeline
- `service.py` - Stage 1: Scenario interpretation and entity extraction
- `subgraph_retrieval.py` - Stage 2: Original single-pass KG retrieval
- `test_generator.py` - Stage 3: Original single-pass test generation
- `example_end_to_end.py` - Original pipeline example

### Multi-Agent System (NEW)
- `multi_agent_retrieval.py` - Multi-agent KG retrieval with intelligent expansion
- `multi_agent_test_generation.py` - Multi-agent test generation with refinement
- `multi_agent_pipeline.py` - Complete orchestrator for multi-agent workflow
- `example_multi_agent.py` - Examples demonstrating the new system
- `MULTI_AGENT_ARCHITECTURE.md` - Comprehensive architecture documentation

---

## Troubleshooting

### Multi-Agent System

**Issue**: Quality threshold not met after max iterations
- Check reviewer feedback in output
- Increase `max_refinement_iterations`
- Lower `quality_threshold` if too strict
- Verify LLM model capability (70B+ recommended)

**Issue**: Too many nodes retrieved
- Reduce `max_iterations` (try 2 instead of 3)
- Increase `min_relevance_score` (try 0.4)
- Reduce `expansion_depth` (try 1)

**Issue**: Slow performance (>60s)
- Reduce retrieval iterations
- Use `breadth_first` instead of `priority_guided`
- Reduce refinement iterations

See [MULTI_AGENT_ARCHITECTURE.md](./MULTI_AGENT_ARCHITECTURE.md) for detailed troubleshooting guide.

---

## Notes
- If Neo4j vector indexes are unavailable, the service builds a local cache of node embeddings from `KGNode` properties: `name`, `description`, `embedding`.
- If a node lacks an `embedding`, one is computed from text using the configured embedding model.
- Vector index search uses `CALL db.index.vector.queryNodes('<index>', k, $embedding)`.
- Multi-agent system requires LangChain and LiteLLM dependencies.
- API costs are minimal (~$0.005-$0.01 per scenario with Groq/Llama)


